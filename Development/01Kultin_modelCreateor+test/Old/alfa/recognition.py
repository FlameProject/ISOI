# recognition.py (–ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è)
import torch
import numpy as np
import cv2
from model import Config

def prepare_for_model_enhanced(char_img, target_size=24, enhance_contrast=True):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–∞ —Å —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    h, w = char_img.shape
    
    # 1. –£—Å–∏–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ –¥–ª—è —Å–∏–º–≤–æ–ª–∞
    if enhance_contrast:
        if np.std(char_img) > 8:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—å –∫–∞–∫–æ–π-—Ç–æ –∫–æ–Ω—Ç—Ä–∞—Å—Ç
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            char_normalized = cv2.normalize(char_img, None, 0, 255, cv2.NORM_MINMAX)
            
            # CLAHE –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if h > 10 and w > 10:  # –¢–æ–ª—å–∫–æ –¥–ª—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))
                char_img = clahe.apply(char_normalized)
            else:
                char_img = char_normalized
    
    # 2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è —Å–∏–º–≤–æ–ª–∞
    if len(np.unique(char_img)) > 2:
        _, binary = cv2.threshold(char_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        char_binary = binary
    else:
        char_binary = char_img
    
    # 3. –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Ç–µ–º–Ω—ã–π –Ω–∞ —Å–≤–µ—Ç–ª–æ–º —Ñ–æ–Ω–µ
    if np.mean(char_binary) > 127:
        char_binary = 255 - char_binary
    
    # 4. –ù–∞—Ö–æ–¥–∏–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–∏–º–≤–æ–ª–∞
    points = cv2.findNonZero(char_binary)
    if points is not None:
        x, y, w_char, h_char = cv2.boundingRect(points)
        
        # –í—ã—Ä–µ–∑–∞–µ–º —Å–∏–º–≤–æ–ª —Å –Ω–µ–±–æ–ª—å—à–∏–º –æ—Ç—Å—Ç—É–ø–æ–º
        margin = 1
        x1 = max(0, x - margin)
        y1 = max(0, y - margin)
        x2 = min(w, x + w_char + margin)
        y2 = min(h, y + h_char + margin)
        
        char_cropped = char_binary[y1:y2, x1:x2]
        h_crop, w_crop = char_cropped.shape
        
        # 5. –†–µ—Å–∞–π–∑ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
        scale = min(target_size / w_crop, (target_size * 0.85) / h_crop)
        new_w = int(w_crop * scale)
        new_h = int(h_crop * scale)
        
        if new_w > 0 and new_h > 0:
            resized = cv2.resize(char_cropped, (new_w, new_h), interpolation=cv2.INTER_AREA)
        else:
            resized = char_cropped
            new_w, new_h = w_crop, h_crop
    else:
        # –ï—Å–ª–∏ —Å–∏–º–≤–æ–ª –ø—É—Å—Ç–æ–π
        resized = np.zeros((target_size, target_size), dtype=np.uint8)
        new_w, new_h = target_size, target_size
    
    # 6. –°–æ–∑–¥–∞–µ–º –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    square = np.zeros((target_size, target_size), dtype=np.uint8)
    
    if points is not None:
        # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª–∏
        x_offset = (target_size - new_w) // 2
        # –°–º–µ—â–∞–µ–º –Ω–µ–º–Ω–æ–≥–æ –≤–Ω–∏–∑
        y_offset = target_size - new_h - 2
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã
        y_offset = max(2, min(y_offset, target_size - new_h - 2))
        
        if y_offset + new_h <= target_size and x_offset + new_w <= target_size:
            square[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
        else:
            # –ï—Å–ª–∏ –Ω–µ –≤–ª–µ–∑–∞–µ—Ç, –ø—Ä–æ—Å—Ç–æ –ø–æ —Ü–µ–Ω—Ç—Ä—É
            y_offset = (target_size - new_h) // 2
            square[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
    
    # 7. –õ–µ–≥–∫–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
    if np.std(square) > 10:
        square = cv2.GaussianBlur(square, (1, 1), 0.5)
    
    # 8. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
    tensor_img = torch.from_numpy(square).float() / 255.0
    tensor_img = (tensor_img - 0.5) / 0.5
    tensor_img = tensor_img.unsqueeze(0).unsqueeze(0)  # [1, 1, 24, 24]
    
    return tensor_img, square

def recognize_characters_enhanced(model, device, config, char_images):
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤"""
    print("üß† –ó–ê–ü–£–°–ö –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø...")
    
    recognized_chars = []
    confidences = []
    processed_images = []
    alternative_chars = []
    
    with torch.no_grad():
        for i, (char_img, bbox_info) in enumerate(char_images):
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            tensor_img, processed_img = prepare_for_model_enhanced(char_img, enhance_contrast=True)
            tensor_img = tensor_img.to(device)
            
            # –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
            output = model(tensor_img)
            probabilities = torch.nn.functional.softmax(output, dim=1)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-3 –≤–∞—Ä–∏–∞–Ω—Ç–∞
            top3_probs, top3_indices = torch.topk(probabilities, 3)
            
            # –û—Å–Ω–æ–≤–Ω–æ–π —Å–∏–º–≤–æ–ª
            main_char_idx = top3_indices[0][0].item()
            main_confidence = top3_probs[0][0].item()
            
            if main_char_idx < len(config.chars):
                main_char = config.chars[main_char_idx]
            else:
                main_char = '?'
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            alternatives = []
            for j in range(1, 3):
                alt_idx = top3_indices[0][j].item()
                alt_prob = top3_probs[0][j].item()
                if alt_idx < len(config.chars):
                    alternatives.append((config.chars[alt_idx], alt_prob))
            
            recognized_chars.append(main_char)
            confidences.append(main_confidence)
            processed_images.append(processed_img)
            alternative_chars.append(alternatives)
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å
            if (i + 1) % 20 == 0 or i == 0 or i == len(char_images)-1:
                print(f"   {i+1}/{len(char_images)}: '{main_char}' ({main_confidence:.2%})")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
    text = reconstruct_text_with_spacing(
        recognized_chars, confidences, [b[1] for b in char_images])
    
    print("‚úÖ –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print(f"   –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(recognized_chars)}")
    print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.mean(confidences):.2%}")
    
    if len(text) <= 100:
        print(f"   –¢–µ–∫—Å—Ç: '{text}'")
    else:
        print(f"   –¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤): '{text[:100]}...'")
    
    return text, recognized_chars, confidences, processed_images, alternative_chars

def reconstruct_text_with_spacing(chars, confidences, bboxes):
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏"""
    if not chars:
        return ""
    
    text_parts = []
    current_word = []
    
    for i, (char, confidence, bbox_info) in enumerate(zip(chars, confidences, bboxes)):
        if i == 0:
            current_word.append(char)
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Å–∏–º–≤–æ–ª–∞–º–∏
            prev_x, prev_y, prev_w, prev_h = bboxes[i-1][:4]
            curr_x, curr_y, curr_w, curr_h = bbox_info[:4]
            
            distance = curr_x - (prev_x + prev_w)
            avg_height = (prev_h + curr_h) / 2
            
            # –ï—Å–ª–∏ –±–æ–ª—å—à–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ - –≤–µ—Ä–æ—è—Ç–Ω–æ –ø—Ä–æ–±–µ–ª
            if distance > avg_height * 0.5:
                text_parts.append(''.join(current_word))
                text_parts.append(' ')
                current_word = [char]
            else:
                current_word.append(char)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–ª–æ–≤–æ
    if current_word:
        text_parts.append(''.join(current_word))
    
    return ''.join(text_parts)